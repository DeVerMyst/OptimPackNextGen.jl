% !TeX spellcheck = en_US
\documentclass[9pt,a4paper,twocolumn]{article}
%\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}

\newcommand{\TheTitle}{Deconvolution}
\newcommand{\TheAuthor}{Éric Thiébaut, Jonathan Léger, Matthew Ozon}
\newcommand{\TheDate}{2nd of October, 2015}

\usepackage{eric-common}
\usepackage[left=10mm, right=10mm, top=20mm, bottom=20mm, columnsep=10mm]{geometry}


% Notations:
\newcommand*{\USigmaVt}{U\,\Sigma\,V\T}
\newcommand*{\USigmaVtp}{\bigl(U\,\Sigma\,V\T\bigr)}

%\newcommand*{\FeasibleSet}{\Set{X}}
\newcommand*{\FeasibleSet}{\Omega_{p}}
\newcommand*{\Lagrangian}{\mathscr{L}}
\newcommand*{\AugmentedLagrangian}{\Lagrangian_\Tag{A}}
\newcommand*{\TSVD}{\operatorname{TSVD}_{p}}

\renewcommand*{\cdot}{\mathord{\,\mathchar"2201\,}}
\renewcommand*{\FT}[1]{\widetilde{#1}}

\title{\TheTitle}
\author{\TheAuthor}
\date{\TheDate}

\sloppy
\begin{document}
\maketitle


\textbf{Keywords:} ;


\section{Introduction}

We want to solve the following quadratic problem:
\begin{equation}
  \min_{x \in \Reals^{n}} \Brace[\big]{
    \Norm{H \cdot x - y}_{W}^2 + \mu \, \Norm{D \cdot x}^2
  } \, ,
  \label{eq:quad-prob}
\end{equation}
where $x \in \Reals^{n}$ are the unknowns while $H \in \Reals^{m \times n}$,
$D \in \Reals^{p \times n}$, $y \in \Reals^{m}$ and $\mu \ge 0$ are given.
The notation $\Norm{u}$ denotes the usual Euclidean norm while $\Norm{u}_{W}$
with $W$ symmetric and semi-definite (\ie $W^\star = W$ with the $\star$
exponent to denote the adjoint and $W \ge 0$) denotes a weighted norm:
\begin{align}
  \Norm{u}^2 &= \Inner{x}{x} \, , \\
  \Norm{u}_{W}^2 &= \Inner{x}{W \cdot x} = \Inner{W \cdot x}{x} \, .
\end{align}
For instance, $y$ is a noisy and blurred image, $H$ is the blur operator, $W$
is the matrix of statistical weights, $D$ is some finite difference operator
and $x$ is the deblurred and denoised image.


The solution of problem~(\ref{eq:quad-prob}) has a closed form:
\begin{equation}
  \estim{x} = \Paren[\big]{
    H\T \cdot W \cdot H + \mu \, D\T \cdot D}^{-1}
    \cdot H\T \cdot W \cdot y \, .
  \label{eq:solution}
\end{equation}
For large size non-stationary problems, the solution $\estim{x}$ in
Eq.~(\ref{eq:solution}) cannot be directly computed and an iterative method
has to be used.

Finding $\estim{x}$ amounts to solving the so-called \emph{normal equations}:
\begin{subequations}
\begin{equation}
  A \cdot x = b \, 
  \label{eq:linear-system}
\end{equation}
with:
\begin{equation}
  A =  H\T \cdot W \cdot H + \mu \, D\T \cdot D \, , 
  \label{eq:lhs-matrix}
\end{equation}
the left hand side (LHS) matrix of the system and:
\begin{equation}
  b = H\T \cdot W \cdot y \, ,
  \label{eq:rhs-vector}
\end{equation}
the right hand side (RHS) vector of the system.
\end{subequations}
 
By construction the LSH matrix $A$ is symmetric and at least nonnegative
semi-definite but most of the time it is positive definite\footnote{If $A$ is only nonnegative
semi-definite, then, providing the initial solution is $x^{0} = 0$, the linear conjugate gradients will find the least norm solution of the normal equations which is $A^{\dagger}\cdot b$ where the exponent $\dagger$ indicates the generalized inverse}.

\oops{Linear conjugate gradients.}


\section{Implementations}

\subsection{Restrictions}

To simplify the implementation, we only consider diagonal weighting:
\begin{equation}
  W = \Diag(w) \, ,
\end{equation}
with $w \in \Reals^{m}_+$ a vector of nonnegative weights.

We are in general interested in processing multi-dimensional data (\eg
images), in this case, the dimensions of the problem are:
\begin{align}
  m &= m_1 \times m_2 \times \ldots \times m_r \, , \\
  n &= n_1 \times n_2 \times \ldots \times n_s \, ,
\end{align}
with $r$ the number of dimensions in the \emph{data space} (output space of
$H$) and $s$ the number of dimensions in the \emph{parameter space} (input
space of $H$).


\subsection{The regularization operator}

First we note that $D$ does not appear alone but only combined with itself as
$D\T \cdot D$.  If we introduce $R = D\T \cdot D$, then a more general form of
the original problem~(\ref{eq:quad-prob}) is:
\begin{equation}
  \min_{x \in \Reals^{n}} \Brace[\big]{
    \Norm{H \cdot x - y}_{W}^2 + \mu \, \Norm{x}_{R}^2
  } \, .
  \label{eq:gen-quad-prob}
\end{equation}
The RHS vector $b$ is unchanged but the LHS matrix becomes:
\begin{equation}
  A =  H\T \cdot W \cdot H + \mu \, R \, . 
  \label{eq:gen-lhs-matrix}
\end{equation}
Technically, we therefore just have to consider the operator $R \in \Reals^{n
\times n}$.

Assuming $D$ is implemented by finite differences, the most simple (yet
general) expression for the regularization is:
\begin{equation}
  \Norm{x}_{R}^2 =
  \sum_{k=1}^{s} \sum_{i}
  \Paren*{\frac{x_{i + \Delta i_k} - x_{i}}{\delta_k}}^2
  \approx \Norm{\nabla x}^2 \, ,
  \label{eq:simple-regul}
\end{equation}
where $s$ is the number of dimensions of $x$, $\Delta i_k$ and $\delta_k$ are
the index offset and the step length along the $k$-th dimension and $\nabla x$
is the spatial gradient of $x$.  Note that the second sum has only to be
carried on the indices $i$ such that $i$ and $i + \Delta i_k$ correspond to
existing elements of $x$.

In the simple expression above, the offset is stationary.  It is formally easy
to represent more general boundary conditions by rewriting the regularization
as follows:
\begin{equation}
  \Norm{x}_{R}^2 =
  \sum_{k=1}^{s} \sum_{i}
  \Paren*{\frac{x_{c_k(i)} - x_{i}}{\delta_k}}^2 \, ,
  \label{eq:regul-with-boundary-conditions}
\end{equation}
where $c_k(i)$ is a function which yields to index of the neighbor of the
$i$-th element of $x$ to consider when computing the finite difference along
the $k$-th dimension.

Putting all together, the regularization term can be expanded as:
\begin{equation}
  \mu \, \Norm{x}_{R}^2 =
  \sum_{k=1}^{s} \alpha_k \sum_{i}
  \Paren*{x_{c_k(i)} - x_{i}}^2 \, ,
  \label{eq:gen-regul}
\end{equation}
with $\alpha_k = \mu/\delta_k^2 \ge 0$.

In this simple form, the quadratic regularization can therefore be completely
specified by the regularization weights $\alpha_k$ along each dimension and
perhaps a few parameters indicating which kind of finite differences and
boundary conditions to consider.  Having possibly different regularization
weights along the dimensions is important to deal with multi-variate data.

For the iterative algorithm, what needs to be implemented is the operator $\mu
\, R$ itself.  The pseudo-code (using Julia notation) is:
\begin{algolist}
\item[] Given $p \in \Reals^{n}$, compute $q = \mu \, R \cdot p$
\begin{verbatim}
    # clear output vector q
    for i in 1:n
        q[i] = 0
    end
    # compute result "in-place"
    for i in 1:n # for each input element
        for k in 1:s # for each dimension
            j = c[k](i) # index of neighbor
            t = alpha[k]*(p[j] - p[i])
            q[i] -= t
            q[j] += t
        end
    end
\end{verbatim}
\end{algolist}
Of course, this general code can be optimized for given combinations of the
number of dimensions $s$ (to unroll the inner loop) and boundary conditions.


\subsection{The model operator}

We initially only consider deconvolution problems for which the model operator
$H$ which performs the convolution by the point spread function (PSF) can be
approximated by a circulant discrete convolution computed by means of the fast
Fourier transform:
\begin{equation}
  H = S \cdot (1/n)\,F^\star \cdot \Diag(F \cdot h) \cdot F
\end{equation}
where $F$ is the multi-dimensional discrete Fourier transform (the FFT), $h$
is the PSF (correctly zero-padded and shifted) and $S \in \Reals^{m \times n}$
is a diagonal operator which extract the part of the blurred model
corresponding to the data.  Note that the adjoint of the FFT, $F^\star$ is
usually called the (unscaled) \emph{backward} FFT and $(1/n)\,F^\star =
F^{-1}$ is the inverse FFT.

Thus the full:
\begin{equation}
  H\T \cdot W \cdot H
  = F^\star \cdot \Diag(\FT{h}^\star) \cdot F \cdot Q \cdot
    F^\star \cdot \Diag(\FT{h}) \cdot F
\end{equation}
where $\FT{h} = F \cdot h$ the FFT of the PSF (the so-called MTF) has to
be computed only once and where:
\begin{equation}
  Q =(1/n^2) \, S\T \cdot W \cdot S
\end{equation}
is mostly a diagonal operator.  The adjoint (transpose) operator of $S$ is a
zero-padding operator.  Thus the above operator involves 4 FFT's.  Also note
that applying $Q$ can be done in a very efficient way \emph{in-place} even
though the out of $F^\star$ and the input of $F$ is of complex type.

\oops{Optimization:} If operator $H\T \cdot W \cdot H$ is applied before $\mu
\, R$, it is not necessary to clear the vector $q$.


Other possibilities:
\begin{itemize}
\item $H = I$ (the identity) to solve denoising problems and interpolation of
missing data (of limited interest with a simple quadratic regularization?).

\item For small PSF, the discrete convolution can be computed directly
(\ie without FFT's).

\item More general small kernels can be implemented via a sparse matrix $H$.

\item \etc

\end{itemize}


\subsection{Coordinates and zero-padding}

When arrays need to be zero-padded or centered for applying the FFT, a
consistent convention must be adopted.  The following one is compatible with
Julia/NumPty/Matlab conventions.

The center of a dimension of length $n$ is at offset $\lfloor n/2 \rfloor$. 
For instance, using coordinate zero to indicate the center, the coordinates
along the axis are:
\begin{verbatim}
   n is even:      [-3|-2|-1| 0|+1|+2]
   n is odd:       [-3|-2|-1| 0|+1|+2|+3]
\end{verbatim}

When zero-padding is performed (\eg from length $n$ to length $n' \ge n$), the
operation should be such that the center in the original array corresponds to
the center in the zero-padded array.  As the center offsets are respectively
$\lfloor n/2 \rfloor$ and $\lfloor n'/2 \rfloor$ in these two arrays, the
number of leading zeros is $\lfloor n'/2 \rfloor - \lfloor n/2 \rfloor$.

With these simple rules, there are no ambiguities in the assumed position of
the center and zero-padding offsets.

\subsection{Inputs}

The inputs of the problem are:
\begin{itemize}
\item the \strong{dimensions} of the problem: $m$, $n$ (and perhaps $p$);

\item the \strong{data} $y \in \Reals^{m}$ in the form of an array of $m$
floating point values or the RHS vector $b \in \Reals^{n}$ (\oops{for a more
general use it is even better to be able to specify $b$ instead of $y$ and
this also saves some in-board memory});

\item the \strong{statistical weights} $w \in \Reals^{m}_+$ in the form of an
array of $m$ floating point nonnegative values;

\item the storage for the (approximate) \strong{solution} $\estim{x} \in
\Reals^{n}$ in the form of an array of $n$ floating point values;

\item the parameters of the \strong{model operator} $H \in \Reals^{m \times
n}$; in the case of the deconvolution, this is just the (zero-padded and
shifted) PSF;

\item the \strong{regularization parameter} $\mu > 0$ and the parameters of
the \strong{regularization operator} $R \in \Reals^{n \times n}$, this amounts
to specifying the $\alpha_k\ge 0$ and the boundary conditions;

\end{itemize}



\end{document}

\section{New formulation}
\oops{Intro: explain the Eckart-Young-Mirsky theorem and TSVD.}

Problem:
\begin{equation}
  \min_{X \in \FeasibleSet} f(X) \, ,
\end{equation}
with $f: \Reals^{m \times n} \mapsto \Reals$ and $\FeasibleSet \subset
\Reals^{m \times n}$ the feasible set which is the subset of rank-$p$
matrices of $\Reals^{m \times n}$ with $p \le \min(m,n)$.

Introduce auxiliary matrix $M \in \Reals^{m \times n}$ and solve the
equivalent problem:
\begin{equation}
  \min_{M, X \in \FeasibleSet} f(M)
  \quad\text{s.t.}\quad
  M = X \, .
\end{equation}

The Lagrangian of the constrained problem is:
\begin{equation}
  \Lagrangian(X,M;G) = f(M) + \Inner{G}{X - M} \, ,
\end{equation}
where $G$ are the Lagrange multipliers associated with the equality
constraint $X = M$ and the inner product of matrices is defined by:
\begin{equation}
  \Inner{X}{Y} = \sum_{i,j} X_{i,j}\,Y_{i,j} \, .
  %\label{eq:inner-prod}
\end{equation}
Equipped with this definition, the Frobenius norm becomes:
\begin{displaymath}
  \Norm{X}_\Tag{F}^2 = \Inner{X}{X} \, .
\end{displaymath}

The Karush-Kuhn-Tucker (KKT) necessary 1st conditions of optimality
are:
\begin{align}
  M^{\star} &= X^{\star} \, ,
  &\text{(KKT1)} \\
  X^{\star} &\in \FeasibleSet \, ,
  &\text{(KKT2)} \\
  G^{\star} &\in \partial f\big(M^{\star}\bigr) \, ,
  &\text{(KKT3)}
\end{align}
where the exponent $\star$ denotes the solution and $\partial f(M)$ is the
sub-differential of $f(M)$.  Note that the KKT conditions characterize a
local (not global) minimum.

To solve the problem in practice, we introduce the augmented Lagrangian:
\begin{equation}
  \AugmentedLagrangian(X,M;G,\rho)
  = \Lagrangian(X,M;G)
   + \frac{\rho}{2} \, \Norm{X - M}_\Tag{F}^2 \, ,
\end{equation}
where $\rho > 0$ is the parameter of the augmented penalty term.

The augmented Lagrangian can be put in the more convenient form:
\begin{align}
  \AugmentedLagrangian(X,M;G,\rho)
  &= f(M) + \frac{\rho}{2} \, \Norm{X - M + G/\rho}_\Tag{F}^2
  \notag \\
  &\quad - \frac{1}{2\,\rho} \, \Norm{G}_\Tag{F}^2 \, .
\end{align}


\subsection{1st ADMM Algorithm}

We propose to solve the constrained problem by the alternating direction
method of multipliers (ADMM):
\begin{algolist}
\item[]\textbf{Algorithm 1.}

\item[0.] Initialization: choose an initial feasible matrix $X^0 \in
  \FeasibleSet$, initial multipliers $G^0$ and let $k=0$.

\item[1.] Choose $\rho^{k}>0$ and update auxiliary matrix $M$:
  \begin{align}
    M^{k+1}
    &= \argmin_{M} \AugmentedLagrangian(X^{k},M;G^{k},\rho^{k}) \notag \\
    &= \argmin_{M} \Brace[\Big]{
      f(M) + \frac{\rho^{k}}{2} \, \Norm{X^{k} - M + G^{k}/\rho^{k}}_\Tag{F}^2
    } \notag \\
    &= \Prox_{f/\rho^{k}}(X^{k} + G^{k}/\rho^{k}) \, ,
  \end{align}
  where $\Prox_{f}(X)$ is the proximal operator of the function $f$ defined by:
  \begin{equation}
    \Prox_{f}(X) \equiv \argmin_{X'} \Brace[\Big]{
      f(X') + \frac{1}{2} \, \Norm{X' - X}_\Tag{F}^2
    } \, .
  \end{equation}

\item[2.] Update $X$ given $M^{k+1}$, $G^{k}$ and $\rho^{k}$:
  \begin{align}
    X^{k+1}
    &= \argmin_{X \in \FeasibleSet}
    \AugmentedLagrangian(X,M^{k+1};G^{k},\rho^{k}) \notag \\
    &= \argmin_{X \in \FeasibleSet}
    \Norm{X - M^{k+1} + G^{k}/\rho^{k}}_\Tag{F} \notag \\
    &= \TSVD(M^{k+1} - G^{k}/\rho^{k}) \, ,
  \end{align}
  where $\TSVD(X)$ yields the least squares $p$-rank approximation of the
  matrix $X$.  In practice this is implemented by means of the TSVD method.

\item[3.] Update multipliers $G$:
  \begin{displaymath}
    G^{k+1}
    = G^{k} + \rho^{k} \, \bigl(
       X^{k+1} - M^{k+1}
    \bigr)
  \end{displaymath}

\item[4.] Until convergence of the algorithm, let $k \leftarrow k+1$
  and proceed with Step~1.

\end{algolist}
In the above algorithm, the exponents denote the iterate.

To check for the convergence of the algorithm, we have to assert whether
the KKT conditions are satisfied at the end of an ADMM iteration.  The
updating of $X$ (Step~2 of Algorithm~1) ensures that $X^{k+1} \in
\FeasibleSet$ and the 2nd KKT condition therefore holds at every iteration.
For the 1st KKT condition to (approximately) hold, the so-called
\emph{primal} residuals:
\begin{equation}
  R^{k} \equiv X^{k+1} - M^{k+1} \, ,
\end{equation}
must be (approximately) equal to zero.  After the updating of the auxiliary
matrix (Step~1 of Algorithm~1), the 1st order optimality condition implies
that:
\begin{displaymath}
  G^{k} + \rho^{k} \, \Paren{X^{k} - M^{k+1}}
  \in \partial f(M^{k+1}) \, ,
\end{displaymath}
which (accounting for the updating of the multipliers) is equivalent to:
\begin{displaymath}
  G^{k+1} - \rho^{k}\,\Paren{X^{k+1} - X^{k}} \in \partial f(M^{k+1}) \, .
\end{displaymath}
Hence, for the 3rd KKT condition to (approximately) hold, the so-called
\emph{dual} residuals:
\begin{equation}
  S^{k} \equiv \rho^{k}\,\Paren{X^{k+1} - X^{k}} \, ,
\end{equation}
must be (approximately) equal to zero.  Considering that the KKT conditions
amounts to requiring that:
\begin{align*}
  &R^{k} = X^{k+1} - M^{k+1} \approx 0 \, , \\
  &G^{k+1} - S^{k} \approx G^{k+1}\, ,
\end{align*}
leads to:
\begin{align*}
  \Norm{R^{k}}_\Tag{F} &\ll \max\Paren{
    \Norm{X^{k+1}}_\Tag{F},
    \Norm{M^{k+1}}_\Tag{F}
  } \, , \\
  \Norm{S^{k}}_\Tag{F} &\ll \Norm{G^{k+1}}_\Tag{F} \, .
\end{align*}
In fact, it is more convenient to define the requested maximum sizes of the
residuals at the start of an ADMM iteration, thus the first condition
above becomes:
\begin{displaymath}
  \Norm{R^{k}}_\Tag{F} \ll \max\Paren{
    \Norm{X^{k}}_\Tag{F},
    \Norm{M^{k}}_\Tag{F}
  } \, .
\end{displaymath}
Putting all these considerations together, convergence of the ADMM
algorithm can be monitored by requiring that the sizes of the \emph{primal}
and \emph{dual} residuals are such that:
\begin{align}
  \Norm{R^{k}}_\Tag{F} &\le \varepsilon_\Tag{prim}^{k} \, , \\
  \Norm{S^{k}}_\Tag{F} &\le \varepsilon_\Tag{dual}^{k} \, ,
\end{align}
where:
\begin{align}
  \varepsilon_\Tag{prim}^{k} &= \varepsilon \, \max\Paren{
    \Norm{X^{k}}_\Tag{F},
    \Norm{M^{k}}_\Tag{F}
  } \, , \\
  \varepsilon_\Tag{dual}^{k} &= \varepsilon \, \Norm{G^{k}}_\Tag{F} \, ,
\end{align}
for some chosen small tolerance $\varepsilon \in (0,1)$.

\oops{Another way to monitor convergence is to compare $f(X^{k+1})$ and
  $f(X^{k})$; but I am not sure that enforcing a monoone decreasing of the
  cost function is a good strategy.}

If $f(X)$ is differentiable, then $\partial f(X) = \{\nabla f(X)\}$ and
the third KKT condition becomes:
\begin{equation}
  G^{\star} = \nabla f(X^{\star}) \, ,
\end{equation}
\oops{and ...}

\subsection{2nd ADMM Algorithm}

The 2nd and 3rd steps of Algorithm~1 can be interverted to obtain a 2nd
ADMM algorithm:
\begin{algolist}
\item[]\textbf{Algorithm 2.}

\item[0.] Initialization: choose an initial auxiliary matrix $M^0$, initial
  multipliers $G^0$ and let $k=0$.

\item[1.] Choose $\rho^{k}>0$ and update $X$:
  \begin{align}
    X^{k+1}
    &= \argmin_{X \in \FeasibleSet}
    \AugmentedLagrangian(X,M^{k};G^{k},\rho^{k}) \notag \\
    %&= \argmin_{X \in \FeasibleSet}
    %\Norm{X - M^{k} + G^{k}/\rho^{k}}_\Tag{F} \notag \\
    &= \TSVD(M^{k} - G^{k}/\rho^{k}) \, .
  \end{align}

\item[2.] Update auxiliary matrix $M$:
  \begin{align}
    M^{k+1}
    &= \argmin_{M} \AugmentedLagrangian(X^{k+1},M;G^{k},\rho^{k}) \notag \\
    %&= \argmin_{M} \Brace[\Big]{
    %  f(M) + \frac{\rho^{k}}{2} \, \Norm{X^{k+1} - M + G^{k}/\rho^{k}}_\Tag{F}^2
    %} \notag \\
    &= \Prox_{f/\rho^{k}}(X^{k+1} + G^{k}/\rho^{k}) \, .
  \end{align}

\item[3.] Update multipliers $G$:
  \begin{displaymath}
    G^{k+1}
    = G^{k} + \rho^{k} \, \bigl(
       X^{k+1} - M^{k+1}
    \bigr)
  \end{displaymath}
\item[4.] Until convergence of the algorithm, let $k \leftarrow k+1$
  and proceed with Step~1.
\end{algolist}

After the updating of the auxiliary matrix (Step~2 of Algorithm~2), the 1st
order optimality condition implies that:
\begin{displaymath}
  G^{k} + \rho^{k} \, \Paren{X^{k+1} - M^{k+1}}
  \in \partial f(M^{k+1}) \, ,
\end{displaymath}
which (accounting for the updating of the multipliers) is equivalent to:
\begin{displaymath}
  G^{k+1} \in \partial f(M^{k+1}) \, .
\end{displaymath}
Hence, the 3rd KKT conditions automatically hold at the end of an iteration
of Algorithm~1.  In Algorithm~2, the \emph{dual} residuals are defined
differently than in Algorithm~1 as:
\begin{equation}
  S^{k} \equiv \rho^{k}\,\Paren{M^{k+1} - M^{k}} \, ,
\end{equation}
\oops{This is dictated by intuition, I do not have found a justification
  for that.}


\subsection{Tuning Augmented Penalty}

We consider here means to tune $\rho$ during the ADMM iterations not only
to ensure the convergence but also to speed up the convergence.  Since the
relative tolerance $\varepsilon$ is the same for the primal and dual
residuals, we naturally would like to chose $\rho^{k}$ such that:
\begin{equation}
  \rho^{k} = \argmin_{\rho > 0} \Brace*{
    \max\Brace*{
      \frac{\Norm{R^{k+}(\rho)}_\Tag{F}}{\tau_\Tag{prim}^{k}},
      \frac{\Norm{S^{k+}(\rho)}_\Tag{F}}{\tau_\Tag{dual}^{k}}
    }
  } \, ,
\end{equation}
with:
\begin{align}
  \tau_\Tag{prim}^{k} &= \max\Paren{
    \Norm{X^{k}}_\Tag{F},
    \Norm{M^{k}}_\Tag{F}
  } \, , \\
  \tau_\Tag{dual}^{k} &= \Norm{G^{k}}_\Tag{F} \, ,
\end{align}
and where $R^{k+}(\rho)$ and $S^{k+}(\rho)$ denote respectively the primal
and dual residuals at the end of the $k$-th ADMM iteration with augmented
penalty parameter equal to $\rho$.  Such a choice of the augmented penalty
parameter ensures the fastest convergence. \oops{Prove that!}

As $\Norm{R^{k+}(\rho)}_\Tag{F}$ and $\Norm{S^{k+}(\rho)}_\Tag{F}$ are
respectively a strictly decreasing and increasing function of $\rho$, the
value $\rho^{k}$ exists and is unique. \oops{Show that and also this is
  true if the limits are 0 and infinite...}

In practice, we do not want to try too many different values of $\rho$ at
every iterations.  We propose the following method to relax the constraint
of accurately solving for $\rho$. (Unless necessary, we drop the
(sub-)iteration number.)
\begin{algolist}
  \item[] \textbf{Tune augmented penalty parameter.}

  \item[0.] Set $\rho_\Tag{min} = 0$, $\rho_\Tag{max} = +\infty$ and
    choose $\rho \in (\rho_\Tag{min}, \rho_\Tag{max})$, for instance
    $\rho=\rho^{k-1}$.

  \item[1.] Perform ADMM iteration and compute:
    \begin{align*}
      \eta_\Tag{prim} &= \frac{\Norm{R^{k+}(\rho)}_\Tag{F}}{\tau_\Tag{prim}^{k}} \\
      \eta_\Tag{dual} &= \frac{\Norm{S^{k+}(\rho)}_\Tag{F}}{\tau_\Tag{dual}^{k}} \, .
    \end{align*}
    Then, if $\eta_\Tag{dual} > (1 + \sigma)\,\eta_\Tag{prim}$, $\rho$ is
    too large so let $\rho_\Tag{max} \leftarrow \rho$ and reduce $\rho$ according
    to:
    \begin{displaymath}
      \rho \leftarrow
      \begin{cases}
        \rho/\gamma & \text{if $\rho_\Tag{min} = 0$,} \\
        \sqrt{\rho_\Tag{min}\,\rho_\Tag{max}} & \text{else;}
      \end{cases}
    \end{displaymath}
    otherwise, if $\eta_\Tag{prim} > (1 + \sigma)\,\eta_\Tag{dual}$, $\rho$
    is too small so let $\rho_\Tag{min} \leftarrow \rho$ and augment $\rho$
    according to:
    \begin{displaymath}
      \rho \leftarrow
      \begin{cases}
        \rho\times\gamma & \text{if $\rho_\Tag{max} = +\infty$,} \\
        \sqrt{\rho_\Tag{min}\,\rho_\Tag{max}} & \text{else;}
      \end{cases}
    \end{displaymath}
    otherwise an acceptable value for the augmented parameter has been found:
    let $\rho^{k} = \rho$ and terminate.
\end{algolist}
where $\sigma\in(0,1)$ is a tolerance parameter and $\gamma > 1$ is a gain.
Once the solution has been bracketed, the above search method is similar to
a bisection algorithm applied to $\log\rho$.

Let us assume that the ratio of the sizes of the resiudals follows the
approximate model:
\begin{displaymath}
  \frac{\Norm{S^{k+}(\rho)}_\Tag{F}}{\Norm{R^{k+}(\rho)}_\Tag{F}}
  \approx \mu \, \rho^{\beta} \, ,
\end{displaymath}
with some unknown $\mu > 0$ and a given $\beta > 0$ (both may be allowed
to vary with the iterations).  Then solving for the unknown $\mu$, we
can update $\rho$ according to:
\begin{equation}
  \rho \leftarrow \Paren*{
    \frac{\eta_\Tag{prim}}{\eta_\Tag{dual}}
  }^{\!\!1/\beta} \times \rho \, .
\end{equation}
This may be an efficient mean to adapt the gain $\gamma$.  \oops{Modify
  this to account for the bounds $\rho_\Tag{min}$ and $\rho_\Tag{max}$.}

\oops{Another possibility is to use Brent's \texttt{fzero} method with
  bracketing.}

\subsection{Inital Solution and Warm Start}


\end{document}
